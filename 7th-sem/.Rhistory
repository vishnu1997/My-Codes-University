wineColor = factor(country)
wineColor
wineColor[1]
wineColor$Levels
wine[-1,]
head(wine[,-1])
head(wine[,1])
head(wine[,1:3])
head(scale(wine[,-1]))
yiu=prcomp(scale(wine[,-1]))
yiu
plot(wine[,2],col=wineClasses)
plot(wine[,2:3],col=wineClasses)
plot(wine[,2],col=wineClasses)
plot(wine[,2:3],col=wineClasses)
plot(wine[,2],col=wineClasses)
plot(yiu[,1],col=wineClasses)
plot(yiu$x[,1],col=wineClasses)
myPca$x
yiu
plot(yiu$x[,3],col=wineClasses)
inputData <- read.csv("http://rstatistics.net/wp-content/uploads/2015/09/adult.csv")
head(inputData)
#Check the class bias
calss = factor(inputData$ABOVE50K)
calss
#Check the class bias
table(inputData$ABOVE50K)
#Check the class bias
table(inputData$SEX)
# Create Training Data
input_ones <- inputData[which(inputData$ABOVE50K == 1), ]  # all 1's
input_ones
input_zeros <- inputData[which(inputData$ABOVE50K == 0), ]  # all 0's
set.seed(100)  # for repeatability of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
head(input_ones_training_rows)
nrow(input_ones_training_rows)
nrow(input_ones)
input_ones_training_rows
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's
# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's
#compute Information values
library(smbinning)
install.packages("smbinning")
# segregate continuous and factor variables
factor_vars <- c ("WORKCLASS", "EDUCATION", "MARITALSTATUS", "OCCUPATION", "RELATIONSHIP", "RACE", "SEX", "NATIVECOUNTRY")
continuous_vars <- c("AGE", "FNLWGT","EDUCATIONNUM", "HOURSPERWEEK", "CAPITALGAIN", "CAPITALLOSS")
iv_df <- data.frame(VARS=c(factor_vars, continuous_vars), IV=numeric(14))  # init for IV results
# compute IV for categoricals
for(factor_var in factor_vars){
smb <- smbinning.factor(trainingData, y="ABOVE50K", x=factor_var)  # WOE table
if(class(smb) != "character"){ # heck if some error occured
iv_df[iv_df$VARS == factor_var, "IV"] <- smb$iv
}
}
wineClasses <- factor(wine$Cvs)
wineClasses
??glm
library(ggplot2)
data("iris")
head(iris,10)
ggplot(iris,aes(Petal.Length,Petal.Width,color = Species))+geom_point()
irisCluster = kmeans(iris[,3:4],3,nstart=10)
head(irisCluster)
re = table(irisCluster$cluster,iris$Species)
ggplot(iris,aes(Petal.Length,Petal.Width,color = irisCluster$cluster))+geom_point()
ggplot(iris,aes(Petal.Length,Petal.Width,color = irisCluster$Species))+geom_point()
#Getting started with Naive Bayes
#Install the package
#install.packages(“e1071”)
#Loading the library
library(e1071)
?naiveBayes #The documentation also contains an example implementation of Titanic dataset
#Next load the Titanic dataset
data("Titanic")
#Save into a data frame and view it
Titanic_df=as.data.frame(Titanic)
#Creating data from table
repeating_sequence=rep.int(seq_len(nrow(Titanic_df)), Titanic_df$Freq) #This will repeat each combination equal to the frequency of each combination
#Create the dataset by row repetition created
Titanic_dataset=Titanic_df[repeating_sequence,]
#We no longer need the frequency, drop the feature
Titanic_dataset$Freq=NULL
#Fitting the Naive Bayes model
Naive_Bayes_Model=naiveBayes(Survived ~., data=Titanic_dataset)
#What does the model say? Print the model summary
Naive_Bayes_Model
#Prediction on the dataset
NB_Predictions=predict(Naive_Bayes_Model,Titanic_dataset)
#Confusion matrix to check accuracy
table(NB_Predictions,Titanic_dataset$Survived)
#Getting started with Naive Bayes in mlr
#Install the package
#install.packages(“mlr”)
#Loading the library
library(mlr)
#Create a classification task for learning on Titanic Dataset and specify the target feature
task = makeClassifTask(data = Titanic_dataset, target = "Survived")
#Initialize the Naive Bayes classifier
selected_model = makeLearner("classif.naiveBayes")
#Train the model
NB_mlr = train(selected_model, task)
#Read the model learned
NB_mlr$learner.model
#Predict on the dataset without passing the target feature
predictions_mlr = as.data.frame(predict(NB_mlr, newdata = Titanic_dataset[,1:3]))
##Confusion matrix to check accuracy
table(predictions_mlr[,1],Titanic_dataset$Survived)
install.packages("mlr")
#Getting started with Naive Bayes
#Install the package
#install.packages(“e1071”)
#Loading the library
library(e1071)
?naiveBayes #The documentation also contains an example implementation of Titanic dataset
#Next load the Titanic dataset
data("Titanic")
#Save into a data frame and view it
Titanic_df=as.data.frame(Titanic)
#Creating data from table
repeating_sequence=rep.int(seq_len(nrow(Titanic_df)), Titanic_df$Freq) #This will repeat each combination equal to the frequency of each combination
#Create the dataset by row repetition created
Titanic_dataset=Titanic_df[repeating_sequence,]
#We no longer need the frequency, drop the feature
Titanic_dataset$Freq=NULL
#Fitting the Naive Bayes model
Naive_Bayes_Model=naiveBayes(Survived ~., data=Titanic_dataset)
#What does the model say? Print the model summary
Naive_Bayes_Model
#Prediction on the dataset
NB_Predictions=predict(Naive_Bayes_Model,Titanic_dataset)
#Confusion matrix to check accuracy
table(NB_Predictions,Titanic_dataset$Survived)
#Getting started with Naive Bayes in mlr
#Install the package
#install.packages(“mlr”)
#Loading the library
library(mlr)
#Create a classification task for learning on Titanic Dataset and specify the target feature
task = makeClassifTask(data = Titanic_dataset, target = "Survived")
#Initialize the Naive Bayes classifier
selected_model = makeLearner("classif.naiveBayes")
#Train the model
NB_mlr = train(selected_model, task)
#Read the model learned
NB_mlr$learner.model
#Predict on the dataset without passing the target feature
predictions_mlr = as.data.frame(predict(NB_mlr, newdata = Titanic_dataset[,1:3]))
##Confusion matrix to check accuracy
table(predictions_mlr[,1],Titanic_dataset$Survived)
Titanic
library(ggplot2)
data(iris)
ggplot(iris,aes(Sepal.Length,Sepal.Width,Species))+geom_point()
ggplot(iris,aes(Sepal.Length,Sepal.Width,color=Species))+geom_point()
data = scale(iris[-5])
plot(data)
hist(plot)
hist(data)
boxplot(data)
sc = (pie/sum(sum))*100
sc = round((pie/sum(sum))*100)
sc = round((pie/sum(pie))*100)
pie = as.numeric(pie)
pi = as.numeric(pie)
#------------------------------
slices = c(10,20,30,40,50)
names =c("aasda","basdas","casd","dasd","easd")
pet=round((slices/(sum(slices)))*100)
pet
pie(pet,lab=names,main="PIE CHART",col = rainbow(length((names))))
sc = round((pi1/sum(pi1))*100)
pi1 = c(10,20,30,30,40,50)
names = c("a","a","a","a","a","a")
sc = round((pi1/sum(pi1))*100)
pie(pi1,lab=names,col=rainbow(length(pi1)))
qqww=data.frame(pi1,names)
rrw = is.numeric(qqww)
rrw
qqww
rrw = as.numeric(qqww)
rrw = is.na(qqww)
rrw
pi1 = c(10,20,30,30,na,50)
qqww=data.frame(pi1,names)
qqww
pi1 = c(10,20,30,30,na,50)
pi1 = c(10,20,30,30,NA,50)
names = c("a","a","a","a","a","a")
qqww=data.frame(pi1,names)
qqww
rrw = is.na(qqww)
rrw
data(mtcars)
dt = mtcars
head(dt)
de = iris
head(de)
ewr = prcomp(scale(de[-4]))
de[-4]
ewr = prcomp(scale(de[-3]))
de[-3]
de[-3]
de[-3,]
de[,-3]
de[,-1]
de[,1]
de[,1:3]
de[,1:4]
ewr = prcomp(scale(de[,1:4]))
plot(ewr,col=de$Species)
wine <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep=",")
head(wine)
# Name the variables
colnames(wine) <- c("Cvs","Alcohol","Malic acid","Ash","Alcalinity of ash", "Magnesium", "Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins", "Color intensity", "Hue", "OD280/OD315 of diluted wines", "Proline")
wineClasses <- factor(wine$Cvs)
wineClasses
yiu=prcomp(scale(wine[,-1]))
yiu
plot(yiu$x[,3],col=wineClasses)
plot(ewr$x,col=de$Species)
ewr$x
plot(ewr$x[,4],col=de$Species)
plot(ewr$x[,3],col=de$Species)
plot(ewr$x[,3],col=de$Species)
plot(ewr$x[,2],col=de$Species)
ww = lm(y~x)
#---------------
y = c(50,120,32,21,123,12)
x = c(12,43,2,4,23,13)
ww = lm(y~x)
qa = c(120,50,10)
predict(ww,qa)
predict(ww,listqa)
for eaxh in qa{
predict(ww,eaxh)
}
for (eaxh in qa){
predict(ww,eaxh)
}
rr=c()
for (eaxh in qa){
rr=cbind(rr,predict(ww,eaxh))
}
for (eaxh in qa){
#rr=cbind(rr,predict(ww,eaxh))
eaxh
}
for (eaxh in qa){
#rr=cbind(rr,predict(ww,eaxh))
print(eaxh)
}
qq = data.frame(qa)
rr=c()
rr=cbind(rr,predict(ww,eaxh))
print(eaxh)
for (eaxh in qq){
rr=cbind(rr,predict(ww,eaxh))
print(eaxh)
}
qq
for (eaxh in qq){
rr=cbind(rr,predict(ww,qa[1]))
print(eaxh)
}
a = data.frame(x=170)
print(predict(ww,a))
for (eaxh in qq){
print(predict(ww,a))
print(eaxh)
}
for (eaxh in qq){
a = data.frame(x = eaxh)
print(predict(ww,a))
print(eaxh)
}
plot(y,x)
plot(y,xabline(ww))
plot(y,x,abline(ww))
plot(y,x,abline(ww),pch=16)
plot(y,x,abline(ww),pch=16,cex=1.3)
plot(y,x,abline(ww),pch=16,cex=1.3,lty=3)
plot(y,x,abline(ww),pch=16,cex=1.3,lty=3)
par(no.readonly = FALSE)
plot(y,x,abline(ww),pch=16,cex=1.3,lty=3)
plot(y,x,abline(ww),pch=16,cex=1.3,lty=3,xlim=c(-20,140),ylim=c(-20,60))
dt
newww=lm(mpg~.)
attach(dt)
newww=lm(mpg~.)
newww=lm(dt$mpg~.)
newww=lmmpg~.,dt)
newww=lm(mpg~.,dt)
wetest=dt[sample(1:nrow(dt),1),]
wetest
wete=wetest[,-1]
wete
oute = predict(newww,wete)
oute#-----------------------------------
library(e1071)
data(Titanic)
Titanic_df = as.data.frame(Titanic)
Titanic_df
repiseq = rep.int(seq_len(nrow(Titanic)),Titanic_df$Freq)
repiseq = rep.int(seq_len(nrow(Titanic_df)),Titanic_df$Freq)
repiseq
titdats = Titanic_df[repiseq,]
titdats
naivebaam = naiveBayes(Survived~.titdats)
naivebaam = naiveBayes(Survived~.,titdats)
er = titdats
er $survied = NULL
naiveBei = naiveBayes(Survived~.,er)
nbpredict = predict(naivebaam,titdats)
nbpredict
nerpre = predict(naiveBei,titdats)
nerpre
nerpre = predict(naiveBei,er)
nerpre
table(nerpre,titdats$Survived)
table(nbpredict,titdats$Survived)
p = table(nerpre,titdats$Survived)
corre = diag(p)
corre
naivebaam = naiveBayes(Species~.,iris)
nbpredict = predict(naivebaam,iris[,1:4])
ewe=table(nbpredict,iris$Species)
corre = diag(ewe)
corre
nbpredict
ewe
nbpredict = predict(naivebaam,iris)
nbpredict
ewe=table(nbpredict,iris$Species)
ewe
#------------------------------
acsv = read.csv("adult.csv",header = TRUE,sep=",")
head(acsv)
head(i1)
i1 = acsv[which(acsv$ABOVE50K==1),]
head(i1)
i0 = acsv[which(acsv$ABOVE50K==0)]
i0 = acsv[which(acsv$ABOVE50K==0),]
ri1 = i1[sample(1:nrow(i1),0.7*nrow(i1)),]
ii1=sample(1:nrow(i1),0.7*nrow(i1))
ri1 = i1[ii1,]
ii0=sample(1:nrow(i0),0.7*nrow(i0))
ri0 = i0[ii0,]
ti = rbind(ri0,ri1)
it1 = i0[-ii0,]
it0 = i1[-ii1,]
td = rbind[i1,i0]
td
ii1=sample(1:nrow(i1),0.7*nrow(i1))
ri1 = i1[ii1,]
ii0=sample(1:nrow(i0),0.7*nrow(i0))
ri0 = i0[ii0,]
ti = rbind(ri0,ri1)
it1 = i0[-ii0,]
it0 = i1[-ii1,]
td = rbind[i1,i0]
td
td = rbind(i1,i0)
td
nrow(acsv)
td = rbind(it0,it1)
td
rela = glm(acsv$ABOVE50K~.,data=acsv)
rela = glm(acsv$ABOVE50K~.,data=ti)
ti = rbind(ri1,ri0)
rela = glm(acsv$ABOVE50K~.,data=ti)
rela = glm(acsv$ABOVE50K~.,data=td)
ri1 = i1[ii1,]
ri1
rela = glm(acsv$ABOVE50K~RELATIONSHIP+AGE+CAPITALISM+OCCUPATION,data=td)
rela = glm(acsv$ABOVE50K~RELATIONSHIP+AGE+CAPITALGAIN+OCCUPATION,data=td)
rela = glm(ABOVE50K~RELATIONSHIP+AGE+CAPITALGAIN+OCCUPATION,data=td)
rela = glm(ABOVE50K~RELATIONSHIP+AGE+CAPITALGAIN+OCCUPATION,data=ti)
eee = predict(rela,td)
head(eee)
sd=table(eee,td$ABOVE50K)
sd
head(sd)
sd=table(td$ABOVE50K,eee)
head(sd)
head(sd,5)
inputData <- read.csv("adult.csv")
head(inputData)
#Check the class bias
table(inputData$ABOVE50K)
# Create Training Data
input_ones <- inputData[which(inputData$ABOVE50K == 1), ]  # all 1's
input_zeros <- inputData[which(inputData$ABOVE50K == 0), ]  # all 0's
set.seed(100)  # for repeatability of samples
input_ones_training_rows <- sample(1:nrow(input_ones), 0.7*nrow(input_ones))  # 1's for training
input_zeros_training_rows <- sample(1:nrow(input_zeros), 0.7*nrow(input_ones))  # 0's for training. Pick as many 0's as 1's
training_ones <- input_ones[input_ones_training_rows, ]
training_zeros <- input_zeros[input_zeros_training_rows, ]
trainingData <- rbind(training_ones, training_zeros)  # row bind the 1's and 0's
# Create Test Data
test_ones <- input_ones[-input_ones_training_rows, ]
test_zeros <- input_zeros[-input_zeros_training_rows, ]
testData <- rbind(test_ones, test_zeros)  # row bind the 1's and 0's
#build logiestic model and predict
logitMod <- glm(ABOVE50K ~ RELATIONSHIP + AGE + CAPITALGAIN + OCCUPATION + EDUCATIONNUM, data=trainingData, family=binomial(link="logit"))
# or
predicted <- predict(logitMod, testData, type="response")  # predicted scores
predicted
#summary of Logistic model
summary(logitMod)
summary(rela)
library(InformationValue)
install.packages("InformationValue")
library(InformationValue)
misClassError(td$ABOVE50K,eee)
sensitivity(td$ABOVE50K,eee)
specificity(td$ABOVE50K,eee)
plot(td$ABOVE50K,eee)
confusionMatrix(td$ABOVE50K,eee)
qwqeconfusionMatrix(td$ABOVE50K,eee)
qwqe=confusionMatrix(td$ABOVE50K,eee)
((qwqe[1,2]+qwqe[2,1])/sum(qwqe))
rela = glm(ABOVE50K~RELATIONSHIP+AGE+CAPITALGAIN+OCCUPATION,data=acsv)
eee = predict(rela,td)
sd=table(td$ABOVE50K,eee)
summary(rela)
library(InformationValue)
misClassError(td$ABOVE50K,eee)
sensitivity(td$ABOVE50K,eee)
specificity(td$ABOVE50K,eee)
plot(td$ABOVE50K,eee)
qwqe=confusionMatrix(td$ABOVE50K,eee)
((qwqe[1,2]+qwqe[2,1])/sum(qwqe))
wine <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep=",")
head(wine)
#----------------------------
head(wine[,-1])
head(factor(wine$V1))
irsC = kmeans(wine[,-1],3,nstart=10)
wt = table(irsC$cluster,wine$V1)
wt
irsC2 = kmeans(iris[,1:4],3,nstart=10)
wt2 = table(irsC2$cluster,iris$Species)
wt
wt2
#----------------------------
library(datasets)
ds = hclust(dist(wine),"cen")
plot(ds)
ds = hclust(dist(iris),"cen")
plot(ds)
ds = hclust(dist(mtcars),"cen")
plot(ds)
ds = hclust(dist([iris[,1:4]),"cen")
ds = hclust(dist(iris[,1:4]),"cen")
plot(ds)
ds = hclust(dist(mtcars[sample(1:nrow(mtcars),7)]),"cen")
ds = hclust(dist(mtcars[sample(1:nrow(mtcars),7),]),"cen")
plot(ds)
#-----------------------------
library(mlbench)
library(caret)
help("caret")
??caret
data("PimaIndiansDiabetes")
cone = trainControl(method="repeatedcv",number=10,repeats = 3)
fit.cart= train(diabetes~.,data=PimaIndiansDiabetes,method="rpart",trControl=cone)
set.seed(7)
fit.lda = train(diabetes~.,data=PimaIndiansDiabetes,method = "lda",trControl=cone)
fit.knn=trainControl(diabetes~.,data=PimaIndiansDiabetes,method = "svmRadical",trControl=cone)
fit.knn=train(diabetes~.,data=PimaIndiansDiabetes,method = "svmRadical",trControl=cone)
fit.knn=train(diabetes~.,data=PimaIndiansDiabetes,method = "svmRadial",trControl=cone)
results = resamples(list(CART=fit.cart,LDA=fit.lda,SVM=fit.knn))
summary(results)
sces = list(x=list(relation="free"),y=list(relation="free"))
bwplot(results,scales=sces)
densityplot(results)
bwplot(results,scales=sces)
densityplot(results)
densityplot(results,scales=sces)
densityplot(results)
densityplot(results,scales=sces)
dotplot(results,scales=sces)
xyplot(results,scales=sces)
parallelplot(results)
parallelplot(results,scales=sces)
spom(results)
splom(results)
diss = diff(results)
diss
summary(diss)
fd = trainControl(method="repeatedcv",number=10,repeats=3)
fit.cart=train(diabetes~.,method = "rpart",trControl=fd,data=PimaIndiansDiabetes)
fit.svm=train(diabetes~.,method = "svmRadial",trControl=fd,data=PimaIndiansDiabetes)
fit.knn=train(diabetes~.,method = "knn",trControl=fd,data=PimaIndiansDiabetes)
fit.rf=train(diabetes~.,method = "rf",trControl=fd,data=PimaIndiansDiabetes)
set.seed(7)
fit.rf=train(diabetes~.,method = "rf",trControl=fd,data=PimaIndiansDiabetes)
set.seed(7)
fit.knn=train(diabetes~.,method = "knn",trControl=fd,data=PimaIndiansDiabetes)
set.seed(7)
fit.svm=train(diabetes~.,method = "svmRadial",trControl=fd,data=PimaIndiansDiabetes)
set.seed(7)
fit.knn=train(diabetes~.,method = "knn",trControl=fd,data=PimaIndiansDiabetes)
pre.ca = predict(fit.cart)
